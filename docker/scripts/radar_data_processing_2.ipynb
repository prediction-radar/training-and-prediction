{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRaN7EsWE-Na"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import datetime\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd()) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important variables that determine performance\n",
    "# When any of these variables are changed, keep track of old variable to revert to if poor performance\n",
    "\n",
    "sequence_length = 5 # Number of .npy files to look back on during training\n",
    "prediction_horizon = 5 # Number of .npy files to predict\n",
    "\n",
    "# Width and height of each square in the grid\n",
    "square_size = 50\n",
    "\n",
    "# Required total of dbZ in sample size\n",
    "required_value_threshold = 2500000\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "early_stopping_patience = 15\n",
    "\n",
    "reduce_lr_patience = 5\n",
    "\n",
    "training_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npy files into a list\n",
    "train_npy_files = sorted(glob(root_dir + 'data/train_npy_files/*.npy'))\n",
    "test_npy_files = sorted(glob(root_dir + 'data/test_npy_files/*.npy'))\n",
    "validation_npy_files = sorted(glob(root_dir + 'data/validation_npy_files/*.npy'))\n",
    "\n",
    "# Define the dimensions of the radar images.  This should never change\n",
    "height, width = 3500, 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_squares(square_size):\n",
    "    if height % square_size != 0 or width % square_size != 0:\n",
    "        raise ValueError(f\"{square_size} is not a common factor of both {height} and {width}.\")\n",
    "\n",
    "    rows = height // square_size\n",
    "    columns = width // square_size\n",
    "    return rows * columns\n",
    "\n",
    "# Assume npy_files, sequence_length, and prediction_horizon are already defined\n",
    "num_squares = compute_total_squares(square_size)\n",
    "\n",
    "def split_single_grid(file, square_size, height, width):\n",
    "    grid = np.load(file).reshape(height, width, 1)\n",
    "    squares = [\n",
    "        grid[i:i+square_size, j:j+square_size]\n",
    "        for i in range(0, height, square_size)\n",
    "        for j in range(0, width, square_size)\n",
    "    ]\n",
    "    return np.array(squares)\n",
    "\n",
    "# Function to split all grids sequentially (no parallelization)\n",
    "def split_all_grids_sequential(npy_files, square_size, height, width):\n",
    "    num_squares_per_grid = (height // square_size) * (width // square_size)\n",
    "    num_files = len(npy_files)\n",
    "\n",
    "    # Preallocate space for all the grids that will be split into squares\n",
    "    all_squares = np.empty((num_files, num_squares_per_grid, square_size, square_size, 1), dtype=np.float32)\n",
    "\n",
    "    # Sequentially process each file and store the result\n",
    "    for idx, file in enumerate(npy_files):\n",
    "        squares = split_single_grid(file, square_size, height, width)\n",
    "        all_squares[idx] = squares\n",
    "\n",
    "    return all_squares\n",
    "\n",
    "\n",
    "def split_into_x_y_squares(all_squares, sequence_length, prediction_horizon):\n",
    "    num_files, num_squares, square_size, _, _ = all_squares.shape\n",
    "    num_samples = (num_files - sequence_length - prediction_horizon + 1) * num_squares\n",
    "\n",
    "    # Pre-allocate arrays for input (X) and output (y) sequences\n",
    "    X = np.empty((num_samples, sequence_length, square_size, square_size, 1), dtype=np.float32)\n",
    "    y = np.empty((num_samples, prediction_horizon, square_size, square_size, 1), dtype=np.float32)\n",
    "\n",
    "    sample_idx = 0\n",
    "    for i in range(num_files - sequence_length - prediction_horizon + 1):\n",
    "        for j in range(num_squares):\n",
    "            # For each sample, take the sequence for that square over time\n",
    "            x_sample = all_squares[i:i+sequence_length, j]\n",
    "            X[sample_idx] = x_sample\n",
    "\n",
    "            y_sample = all_squares[i+sequence_length:i+sequence_length+prediction_horizon, j]\n",
    "            y[sample_idx] = y_sample\n",
    "\n",
    "            sample_idx += 1\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Pre-split all grids into squares sequentially\n",
    "train_all_squares = split_all_grids_sequential(train_npy_files, square_size=square_size, height=height, width=width)\n",
    "test_all_squares = split_all_grids_sequential(test_npy_files, square_size=square_size, height=height, width=width)\n",
    "validation_all_squares = split_all_grids_sequential(validation_npy_files, square_size=square_size, height=height, width=width)\n",
    "\n",
    "# Split the pre-split squares into sequences for training, testing, and validation\n",
    "X_train, y_train = split_into_x_y_squares(train_all_squares, sequence_length=sequence_length, prediction_horizon=prediction_horizon)\n",
    "X_test, y_test = split_into_x_y_squares(test_all_squares, sequence_length=sequence_length, prediction_horizon=prediction_horizon)\n",
    "X_validation, y_validation = split_into_x_y_squares(validation_all_squares, sequence_length=sequence_length, prediction_horizon=prediction_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_by_max_value(X, y):\n",
    "    valid_indices = []\n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Sum the pixel values for both X[i] and y[i]\n",
    "        x_sum = np.sum(X[i])\n",
    "        y_sum = np.sum(y[i])\n",
    "        \n",
    "        # If the sum is greater than or equal to the threshold, keep this sample\n",
    "        if x_sum + y_sum >= required_value_threshold:\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    # Select only the valid indices for X and y\n",
    "    X_filtered = X[valid_indices]\n",
    "    y_filtered = y[valid_indices]\n",
    "    \n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "\n",
    "X_train_unique, y_train_unique = remove_duplicates_by_max_value(X_train, y_train)\n",
    "X_test_unique, y_test_unique = remove_duplicates_by_max_value(X_test, y_test)\n",
    "X_validation_unique, y_validation_unique = remove_duplicates_by_max_value(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train_normalized = X_train_unique / 80\n",
    "y_train_normalized = y_train_unique / 80\n",
    "\n",
    "X_test_normalized = X_test_unique / 80\n",
    "y_test_normalized = y_test_unique / 80\n",
    "\n",
    "X_validation_normalized = X_validation_unique / 80\n",
    "y_validation_normalized = y_validation_unique / 80\n",
    "\n",
    "print(X_train_normalized.shape)\n",
    "print(y_train_normalized.shape)\n",
    "print(X_test_normalized.shape)\n",
    "print(y_test_normalized.shape)\n",
    "print(X_validation_normalized.shape)\n",
    "print(y_validation_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colors for the radar map, introducing white for values between 0 and 15\n",
    "colors = [\n",
    "    (0, 0, 0),         # White for values 0-15 (no precipitation or very light)\n",
    "    (0, 0.7, 0),       # Green (light precipitation)\n",
    "    (1, 1, 0),         # Yellow (moderate precipitation)\n",
    "    (1, 0.65, 0),      # Orange (heavy precipitation)\n",
    "    (1, 0, 0),         # Red (very heavy precipitation)\n",
    "    (0.6, 0, 0.6)      # Purple (extreme precipitation)\n",
    "]\n",
    "\n",
    "breakpoints = [0.0, .15/1.0, .40/1.0, .60/1.0, .70/1.0, 1.0]\n",
    "\n",
    "# Create the custom colormap\n",
    "radar_cmap = LinearSegmentedColormap.from_list('radar', colors, N=80)\n",
    "\n",
    "# Normalize the data range from 0 to 80\n",
    "norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "# Construct a figure to visualize the images\n",
    "x_fig, x_axes = plt.subplots(1, 5, figsize=(20, 14))\n",
    "y_fig, y_axes = plt.subplots(1, 5, figsize=(20, 14))\n",
    "\n",
    "# Randomly choose a data example to visualize\n",
    "data_choice = np.random.choice(range(len(X_train_normalized)), size=1)[0]\n",
    "lastdata = None  # Initialize lastdata to None before looping\n",
    "\n",
    "# Plot each of the sequential images for one random data example\n",
    "for idx, ax in enumerate(x_axes.flat):\n",
    "    x_thisdata = X_train_normalized[data_choice][idx]\n",
    "\n",
    "    # Compare the current data to the last one if lastdata is not None\n",
    "    if lastdata is not None and np.array_equal(x_thisdata, lastdata):\n",
    "        print(f\"Frame x - {idx + 1} is the same as the previous frame.\")\n",
    "\n",
    "    # Display the image with the radar colormap and normalization\n",
    "    im = ax.imshow(np.squeeze(x_thisdata), cmap=radar_cmap, norm=norm)\n",
    "    \n",
    "    # Add title and remove axis\n",
    "    ax.set_title(f\"Frame X - {idx + 1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Update lastdata\n",
    "    lastdata = x_thisdata\n",
    "\n",
    "# Print information and display the figure\n",
    "print(f\"Displaying frames for example {data_choice}.\")\n",
    "\n",
    "for idx, ax in enumerate(y_axes.flat):\n",
    "    y_thisdata = y_train_normalized[data_choice][idx]\n",
    "\n",
    "    # Compare the current data to the last one if lastdata is not None\n",
    "    if lastdata is not None and np.array_equal(y_thisdata, lastdata):\n",
    "        print(f\"Frame y - {idx + 1} is the same as the previous frame.\")\n",
    "\n",
    "    # Display the image with the radar colormap and normalization\n",
    "    im = ax.imshow(np.squeeze(y_thisdata), cmap=radar_cmap, norm=norm)\n",
    "    \n",
    "    # Add title and remove axis\n",
    "    ax.set_title(f\"Frame y - {idx + 1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Update lastdata\n",
    "    lastdata = y_thisdata\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, BatchNormalization, Input, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import subprocess\n",
    "\n",
    "channels = 1  # Reflectivity is your feature, so 1 channel\n",
    "\n",
    "# Define the model using an Input layer for the input shape\n",
    "model = Sequential()\n",
    "\n",
    "# Add Input Layer\n",
    "model.add(Input(shape=(sequence_length, square_size, square_size, channels))) \n",
    "\n",
    "# First ConvLSTM2D layer with return_sequences=True\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Second ConvLSTM2D layer with return_sequences=True to return all frames\n",
    "model.add(ConvLSTM2D(filters=128, kernel_size=(3, 3), padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Third ConvLSTM2D layer\n",
    "model.add(ConvLSTM2D(filters=256, kernel_size=(3, 3), padding='same', return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Replace Conv3D with Conv2D to predict the next frame(s)\n",
    "model.add(Conv3D(filters=1, kernel_size=(3, 3, 3), activation='linear', padding='same'))\n",
    "\n",
    "optimizer = AdamW(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))\n",
    "model_name = f\"model_{training_id}.keras\"\n",
    "\n",
    "# model = load_model(root_dir + f\"model/{model_name}\")\n",
    "# # Create an Adam optimizer with a custom learning rate\n",
    "# optimizer = AdamW()  # Set the learning rate here\n",
    "\n",
    "# # Compile the model with the optimizer\n",
    "# model.compile(loss='mae', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    features_mem = 0  # Initialize memory for features\n",
    "    float_bytes = 4.0  # Float32 uses 4 bytes\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        # Use layer.output.shape to get the output shape instead of output_shape\n",
    "        out_shape = layer.output.shape\n",
    "        \n",
    "        # Remove the batch size dimension (out_shape[0]) and None (which represents the batch dimension)\n",
    "        out_shape = [dim for dim in out_shape if dim is not None]\n",
    "        \n",
    "        # Multiply all output shape dimensions to calculate the number of elements per layer\n",
    "        single_layer_mem = 1\n",
    "        for s in out_shape:\n",
    "            single_layer_mem *= s\n",
    "            \n",
    "        # Convert to memory (in bytes and MB)\n",
    "        single_layer_mem_float = single_layer_mem * float_bytes  # Multiply by 4 bytes (float32)\n",
    "        single_layer_mem_MB = single_layer_mem_float / (1024 ** 2)  # Convert to MB\n",
    "        \n",
    "        print(f\"Memory for layer {layer.name} with output shape {out_shape} is: {single_layer_mem_MB:.2f} MB\")\n",
    "        \n",
    "        features_mem += single_layer_mem_MB  # Accumulate total feature memory\n",
    "    \n",
    "    # Calculate Parameter memory\n",
    "    trainable_wts = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
    "    non_trainable_wts = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
    "    parameter_mem_MB = ((trainable_wts + non_trainable_wts) * float_bytes) / (1024 ** 2)\n",
    "    \n",
    "    print(\"_________________________________________\")\n",
    "    print(f\"Memory for features in MB is: {features_mem * batch_size:.2f} MB\")\n",
    "    print(f\"Memory for parameters in MB is: {parameter_mem_MB:.2f} MB\")\n",
    "\n",
    "    total_memory_MB = (batch_size * features_mem) + parameter_mem_MB\n",
    "    total_memory_GB = total_memory_MB / 1024  # Convert to GB\n",
    "    \n",
    "    return total_memory_GB\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "mem_for_my_model = get_model_memory_usage(batch_size, model)\n",
    "\n",
    "print(\"_________________________________________\")\n",
    "print(\"Minimum memory required to work with this model is: %.2f\" %mem_for_my_model, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint(root_dir + f\"model/{training_id}/{model_name}\", save_best_only=True)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=early_stopping_patience,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=reduce_lr_patience,\n",
    "    factor=0.5,\n",
    "    min_lr=1e-7,\n",
    "    min_delta=0.0001,\n",
    "    cooldown=2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_normalized, y_train_normalized, \n",
    "          batch_size=batch_size, \n",
    "          epochs=training_epochs, \n",
    "          callbacks=[cp, early_stopping, reduce_lr],\n",
    "          validation_data=(X_validation_normalized, y_validation_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model saved by ModelCheckpoint\n",
    "model_path = root_dir + f\"model/{training_id}/{model_name}\"\n",
    "\n",
    "val_loss = history.history['val_loss']  # List of validation losses for each epoch\n",
    "average_val_loss = sum(val_loss) / len(val_loss)  # Compute the average val_loss\n",
    "num_epochs = len(val_loss)  # Number of epochs is the length of the val_loss list\n",
    "\n",
    "# Create the message including average val_loss and number of epochs\n",
    "training_results_message = f\"Trained model: {model_name} with avg_val_loss: {average_val_loss:.4f} epochs: {num_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_output = io.StringIO()\n",
    "with redirect_stdout(mem_output):\n",
    "    mem_for_my_model = get_model_memory_usage(batch_size, model)\n",
    "mem_output_lines = mem_output.getvalue().splitlines()\n",
    "\n",
    "current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "results_lines = [\n",
    "    \"## What I tried new ##\",\n",
    "    \"\",\n",
    "    f\"Model trained on: {current_time}\",\n",
    "    \"\",\n",
    "    training_results_message,\n",
    "    \"\",\n",
    "    \"## Variables ##\",\n",
    "    f\"sequence_length: {sequence_length}\",\n",
    "    f\"prediction_horizon: {prediction_horizon}\",\n",
    "    f\"square_size: {square_size}\",\n",
    "    f\"batch_size: {batch_size}\",\n",
    "    f\"early_stopping_patience: {early_stopping_patience}\",\n",
    "    f\"reduce_lr_patience: {reduce_lr_patience}\",\n",
    "    f\"training_epochs: {training_epochs}\",\n",
    "    f\"required_value_threshold: {required_value_threshold}\",\n",
    "    \"\",\n",
    "    \"## Data Shape ##\",\n",
    "    f\"X_train: {X_train_normalized.shape}\",\n",
    "    f\"y_train: {y_train_normalized.shape}\",\n",
    "    f\"X_test: {X_test_normalized.shape}\",\n",
    "    f\"y_test: {y_test_normalized.shape}\",\n",
    "    f\"X_validation: {X_validation_normalized.shape}\",\n",
    "    f\"y_validation: {y_validation_normalized.shape}\",\n",
    "    \"\",\n",
    "    \"## Memory Needed Summary ##\",\n",
    "    f\"Memory required for model: {mem_for_my_model:.2f} GB\",\n",
    "    *mem_output_lines,\n",
    "]\n",
    "\n",
    "results_filename = f\"results_{training_id}.txt\"\n",
    "results_filepath = os.path.join(root_dir, f\"model/{training_id}\", results_filename)\n",
    "\n",
    "with open(results_filepath, \"w\") as f:\n",
    "    f.write(\"\\n\".join(results_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(known_frames, should_be_frames):\n",
    "    # Predict future frames\n",
    "    predicted_frames = model.predict(np.expand_dims(known_frames, axis=0))\n",
    "    predicted_frames = np.squeeze(predicted_frames, axis=0)\n",
    "    \n",
    "    # Construct a figure for the original and predicted frames (5 rows of 3 columns each)\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(10, 7))  # 5 rows for original + 5 rows for predicted, 3 columns each\n",
    "    \n",
    "    for idx in range(5):\n",
    "        row, col = divmod(idx, 5)\n",
    "        ax = axes[row, col]\n",
    "        ax.imshow(np.squeeze(known_frames[idx]), cmap=radar_cmap, norm=norm)\n",
    "        ax.set_title(f\"Original {idx + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    for idx in range(5):\n",
    "        row, col = divmod(idx, 5)\n",
    "        ax = axes[row + 1, col]  # Offset by 5 rows to separate original and predicted\n",
    "        ax.imshow(np.squeeze(predicted_frames[idx]), cmap=radar_cmap, norm=norm)\n",
    "        ax.set_title(f\"Predicted {idx + 6}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for idx in range(5):\n",
    "        row, col = divmod(idx, 5)\n",
    "        ax = axes[row + 2, col]  # Offset by 5 rows to separate original and predicted\n",
    "        ax.imshow(np.squeeze(should_be_frames[idx]), cmap=radar_cmap, norm=norm)\n",
    "        ax.set_title(f\"Actual {idx + 6}\")\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    # Display the figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainChoice = np.random.choice(range(len(X_train_normalized)), size=1)[0]\n",
    "trainFrames = X_train_normalized[trainChoice]\n",
    "actual_train_frames = y_train_normalized[trainChoice]\n",
    "show_results(trainFrames, actual_train_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valChoice = np.random.choice(range(len(X_validation_normalized)), size=1)[0]\n",
    "val_frames = X_validation_normalized[valChoice]\n",
    "val_actual_frames = y_validation_normalized[valChoice]\n",
    "show_results(val_frames, val_actual_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testChoice = np.random.choice(range(len(X_test_normalized)), size=1)[0]\n",
    "test_frames = X_test_normalized[testChoice]\n",
    "actual_test_frames = y_test_normalized[testChoice]\n",
    "show_results(test_frames, actual_test_frames)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "radarenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
